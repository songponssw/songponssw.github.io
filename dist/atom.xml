<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Home</title><description>Rediscory the beauty of typography</description><link>https://astro-theme-typography.vercel.app/</link><item><title>SpMV Performance and Pages</title><link>https://astro-theme-typography.vercel.app/posts/spmv-lock/</link><guid isPermaLink="true">https://astro-theme-typography.vercel.app/posts/spmv-lock/</guid><description>Note taking for analyzing the page mechanism impats on SpMV performance</description><pubDate>Thu, 11 Dec 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Before we begin&lt;/h2&gt;
&lt;p&gt;I have been researched with the Sparse Matrix-Vector Multiplication (SpMV) for years. Apart from the main research goal, there are some subtle experiments which I really interested in, but somehow they quit a re-research or may not craft to be a major contribution yet.&lt;/p&gt;
&lt;p&gt;This post will discuss about some of my experiments on the SpMV performance and its data locality (at the page level) which is constructed as followed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SpMV locality problem&lt;/li&gt;
&lt;li&gt;How mlock() would help to improve SpMV performance&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer !!!&lt;/strong&gt;
This post is my note-taking and my opinion. It may not provide the correct information and may have the flaw in certain parts. So, please keep in mind that is just one aspect that I observed on the SpMV performance difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2&gt;SpMV Data Locality&lt;/h2&gt;
&lt;p&gt;In context of SpMV (y=Ax), data locality refer to the use of elements in the dense vector X during the computation. As the matrix layout (how elements are placed in matrix) affect the access pattern on elements in the dense vector, the difference matrix layout would produce different data locality even they have the same amount of non-zero elemetns in the matrix.&lt;/p&gt;
&lt;p&gt;The fundamental idea of this experiment is trying the improve the locality of the dense vector X. Ideally, pinning all the values in the last-level of cache (LLC) would improved data locality, reducing the latency.&lt;/p&gt;
&lt;h2&gt;mlock()&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;mlock() locks pages in the address range starting at &lt;em&gt;addr&lt;/em&gt; and continuing for &lt;em&gt;size&lt;/em&gt; bytes. All pages that contain a part of the specified address range are guaranteed to be resident in RAM when the call returns successfully; the pages are guaranteed to stay in RAM until later unlocked. -- &lt;a href=&quot;https://man7.org/linux/man-pages/man2/mlock.2.html&quot;&gt;Linux man-pages&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Okay! &lt;code&gt;mlock()&lt;/code&gt; does not pin the data in the LCC, Here is my assumption&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If the input matrix is large enough and cause the swap mechanism, using mlock() may show the benefit performance than baseline (no mock() used )&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately ChatGPT told us the may not show the performance differences because paging may not show significant SpMV performance improvement. However, just try to measure it anyway.&lt;/p&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;p&gt;I measured the performance on five matrices whose working set size (WSS) is larger than the Last-Level Cache (LLC) (hoping they would allocate more pages). As expected, the speedup between the baseline and using &lt;code&gt;mlock()&lt;/code&gt; is slightly differentâ€”less than 1%. However, most of them perform slower than the baseline. In my opinion, this case should be considered as having no performance difference, since these are likely caused by tiny interferences or measurement errors, which are very small. I would expect the performance improvement impact to be up to around 10%.&lt;/p&gt;
&lt;h4&gt;Does &lt;code&gt;mlock()&lt;/code&gt; help?&lt;/h4&gt;
&lt;p&gt;To show the effects of &lt;code&gt;mlock()&lt;/code&gt;, I measured the performance counter metrics as shown in the table. With &lt;code&gt;mlock()&lt;/code&gt;, we expect that the pages of the dense vector X would be found in RAM, which should reduce the number of cache misses. I collected the number of loads and misses to calculate the miss rate (&lt;code&gt;#miss/#loads&lt;/code&gt;) for LLC, iTLB, and dTLB.&lt;/p&gt;
&lt;p&gt;Again, the LLC miss rate does not show huge differences, except for matrix C, which has a lower LLC miss rate when using &lt;code&gt;mlock()&lt;/code&gt;. We observe that for this matrix, the difference comes from the variation in total LLC loads. The number of LLC misses is almost similar, but the total number of LLC loads differs significantly. As a result, the miss rate difference appears.&lt;/p&gt;
&lt;p&gt;The dTLB and iTLB miss results are harder to correlate with the impact of &lt;code&gt;mlock()&lt;/code&gt; on performance. The TLB handles the translation between virtual and physical pages, and its behavior might not directly reflect the effects of page locking.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Matrix&lt;/th&gt;
&lt;th&gt;Speedup&lt;/th&gt;
&lt;th&gt;LLC Miss rate &amp;lt;br&amp;gt;difference&lt;/th&gt;
&lt;th&gt;dTLB Miss rate &amp;lt;br&amp;gt;difference&lt;/th&gt;
&lt;th&gt;iTLB Miss rate &amp;lt;br&amp;gt;difference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;1.01874&lt;/td&gt;
&lt;td&gt;0.10512&lt;/td&gt;
&lt;td&gt;-0.00015&lt;/td&gt;
&lt;td&gt;-26.97127178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;0.99649&lt;/td&gt;
&lt;td&gt;0.86082&lt;/td&gt;
&lt;td&gt;0.00044&lt;/td&gt;
&lt;td&gt;8.129643415&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;0.99913&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;14.68748&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.00019&lt;/td&gt;
&lt;td&gt;-8.197961915&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;D&lt;/td&gt;
&lt;td&gt;0.99805&lt;/td&gt;
&lt;td&gt;-0.70907&lt;/td&gt;
&lt;td&gt;0.00093&lt;/td&gt;
&lt;td&gt;1782.132078&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;0.99810&lt;/td&gt;
&lt;td&gt;-1.86659&lt;/td&gt;
&lt;td&gt;-0.00045&lt;/td&gt;
&lt;td&gt;-7553.93233&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Difference = baseline - mlock. Minus values would indicates mlock() implemention is better.
&lt;a&gt;Result and code&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion and Limitation&lt;/h2&gt;
&lt;p&gt;The impact of &lt;code&gt;mlock()&lt;/code&gt; is tiny. OS paging is not the primary bottleneck for SpMV. It is difficult to elaborate on the potential factors affecting performance due to the very small performance differences observed. One scenario that might show clearer benefits from page locking is when using a very large matrix with a WSS that causes swapping. In that case, we might see the impact of &lt;code&gt;mlock()&lt;/code&gt; more clearly.&lt;/p&gt;
&lt;p&gt;I know that we may not be able to explain everything happening during computation, as there are many abstractions and optimizations that interplay with performance. This post simply tries to present and interpret some results that may affect SpMV performance in the context of the paging mechanism.&lt;/p&gt;
&lt;h2&gt;What&apos;s next?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Memory allocation: &lt;code&gt;mmap()&lt;/code&gt; vs &lt;code&gt;malloc()&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;Streaming the input matrix A?&lt;/li&gt;
&lt;/ul&gt;
</content:encoded><category>Performance</category><category>SpMV</category><author>songponssw</author></item></channel></rss>